{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFsMH6sWjqjq"
      },
      "source": [
        "# Introduction to Large Language Models (LLMs) and Training Dataset\n",
        "\n",
        "Large Language Models (LLMs) are advanced neural networks designed to understand and generate human language by learning from vast amounts of text data. These models capture complex patterns and context in language, enabling tasks such as text completion, translation, and summarization.\n",
        "\n",
        "To train our LLM from scratch, we start by loading a training dataset. For this project, we intentionally use a relatively small dataset. This choice is driven by two main reasons:\n",
        "\n",
        "- It allows the code to run efficiently on a standard laptop without requiring a powerful GPU.\n",
        "- The training process completes quickly — in just a few minutes rather than weeks — making it ideal for educational purposes and experimentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiI_6F6qFvIq"
      },
      "source": [
        "# Loading the Training Dataset\n",
        "\n",
        "This code snippet handles downloading and loading our training dataset of bedtime stories:\n",
        "\n",
        "- It first checks if the file `gutenberg.txt` already exists locally.\n",
        "- If the file is not found, it downloads the text data from the specified URL and saves it as `gutenberg.txt`.\n",
        "- If the file already exists, it simply loads the data from the local file to avoid redundant downloads.\n",
        "\n",
        "This ensures efficient use of resources by only downloading the dataset once and reusing it for subsequent runs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gnhqrPOxi34z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "file_path = \"gutenberg.txt\"\n",
        "url = \"https://qyxai.com/book_examples/bedtimestories.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        text_data = response.read().decode('utf-8')\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text_data)\n",
        "else:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gnc7jzsGBGJ"
      },
      "source": [
        "# Extracting a Subset of the Dataset\n",
        "\n",
        "In this step, we slice the loaded text data to focus on a specific portion of the dataset for training:\n",
        "\n",
        "- We extract the text from character index 30,707 up to 160,645.\n",
        "- This helps to remove any unwanted headers, footers, or irrelevant parts from the original file.\n",
        "- We then print the first 99 characters and the last 99 characters of this extracted segment to verify the content.\n",
        "\n",
        "This ensures that the dataset used for training is clean and relevant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0JO5RV4O9sD",
        "outputId": "26b476b8-a2e2-46ac-aa46-e5867a6db66b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "old apple tree at the edge of Farmer Giles’ meadow. His home was snug, lined with dandelion fluff a\n",
            "rked, their movements graceful and serene.\n",
            "\n",
            "Mia watched, fascinated. It was like seeing a magical r\n"
          ]
        }
      ],
      "source": [
        "text_data = text_data[30707:160645]\n",
        "\n",
        "print(text_data[:99])\n",
        "\n",
        "print(text_data[-99:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHbSbG_5j_CH"
      },
      "source": [
        "# Tokenization and Counting Tokens\n",
        "\n",
        "Tokenization is the process of splitting text into smaller units called tokens, which can be words, subwords, or characters. Language models like GPT-2 work with tokens rather than raw text because tokens provide a standardized and efficient way to represent language data.\n",
        "\n",
        "In the code snippet below:\n",
        "\n",
        "- We use the `tiktoken` library to load the GPT-2 tokenizer, which follows the specific tokenization rules used by the GPT-2 model.\n",
        "- The `encode` method converts the raw text into a sequence of tokens.\n",
        "- We then calculate the total number of tokens in the dataset, which gives us insight into the dataset size from the model’s perspective.\n",
        "\n",
        "Understanding the token count is essential since language models process and learn from text based on these tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvYPZenMjKiV",
        "outputId": "44529dfb-86b8-4c67-8bb1-21565aa9b7c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: 30091\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "print(\"Tokens:\", total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZm-lrG4l_3O"
      },
      "source": [
        "# Preparing the Dataset and DataLoader for Training\n",
        "\n",
        "To train our LLM to predict the next token in a sequence, we first split the dataset into training and validation sets. We then use PyTorch’s `Dataset` and `DataLoader` classes to create batches suitable for model training.\n",
        "\n",
        "Key points about this process:\n",
        "\n",
        "- We tokenize the entire text using the GPT-2 tokenizer.\n",
        "- The dataset is divided into overlapping chunks (sliding windows) of a fixed length (`max_length`), with a defined stride to control overlap.\n",
        "- Each chunk consists of an input sequence and a target sequence, where the target is the same as the input but shifted by one token to represent the \"next token\" prediction task.\n",
        "- The custom `GPTDatasetV1` class handles this chunking and prepares tensors for inputs and targets.\n",
        "- The `create_dataloader_v1` function wraps this dataset into a PyTorch `DataLoader`, which manages batching, shuffling, and parallel data loading.\n",
        "\n",
        "This setup efficiently prepares the text data for training the language model to learn next-token prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ji-YDiWjdQI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX_lyUuopzZx"
      },
      "source": [
        "# GPT Model Configuration Parameters (124M Version)\n",
        "\n",
        "This dictionary defines the hyperparameters for the GPT model variant with approximately 124 million parameters:\n",
        "\n",
        "- **`vocab_size` (50257):**  \n",
        "  The size of the model’s vocabulary, representing the total number of unique tokens the model can recognize and generate.\n",
        "\n",
        "- **`context_length` (256):**  \n",
        "  The maximum length of the input sequence (in tokens) the model can process at once. Here, it's shortened from the original 1024 tokens to speed up training and reduce memory usage.\n",
        "\n",
        "- **`emb_dim` (768):**  \n",
        "  The dimension of the token embeddings and hidden representations within the model. This defines the size of the vector space in which tokens are represented.\n",
        "\n",
        "- **`n_heads` (12):**  \n",
        "  The number of attention heads in each multi-head self-attention layer. Multiple heads allow the model to focus on different parts of the input simultaneously.\n",
        "\n",
        "- **`n_layers` (12):**  \n",
        "  The number of transformer blocks stacked in the model. Each layer contains attention and feed-forward sub-layers.\n",
        "\n",
        "- **`drop_rate` (0.1):**  \n",
        "  Dropout rate applied during training to prevent overfitting by randomly zeroing out parts of the neural network.\n",
        "\n",
        "- **`qkv_bias` (False):**  \n",
        "  Indicates whether to include bias terms in the query, key, and value projections in the attention mechanism.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkhJhFkdp0v7"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,   # Vocabulary size\n",
        "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
        "    \"emb_dim\": 768,        # Embedding dimension\n",
        "    \"n_heads\": 12,         # Number of attention heads\n",
        "    \"n_layers\": 12,        # Number of layers\n",
        "    \"drop_rate\": 0.1,      # Dropout rate\n",
        "    \"qkv_bias\": False      # Query-key-value bias\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qLmBa9uHZvy"
      },
      "source": [
        "# Splitting Dataset and Creating DataLoaders for Training and Validation\n",
        "\n",
        "- We split the dataset into training and validation sets using a 90%/10% ratio.\n",
        "- `split_idx` calculates the index at which to split the text data accordingly.\n",
        "- The training data is everything before the split index, and validation data is everything after.\n",
        "\n",
        "Next, we create PyTorch DataLoaders for both sets using the previously defined `create_dataloader_v1` function:\n",
        "\n",
        "- For the training loader:\n",
        "  - Batch size is set to 2.\n",
        "  - The sequence length (`max_length`) and stride are set according to the GPT model’s context length (256 tokens).\n",
        "  - `drop_last=True` ensures incomplete batches are dropped for consistent batch sizes.\n",
        "  - Shuffling is enabled to improve training robustness.\n",
        "  \n",
        "- For the validation loader:\n",
        "  - Batch size is also 2.\n",
        "  - The same sequence length and stride are used.\n",
        "  - `drop_last=False` allows processing all validation data.\n",
        "  - Shuffling is disabled for deterministic evaluation.\n",
        "\n",
        "Setting a fixed random seed (`torch.manual_seed(123)`) ensures reproducibility of data shuffling and batch sampling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXlQsn8kFYtU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aec8fq3PFYRF"
      },
      "outputs": [],
      "source": [
        "# Train/validation ratio\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku0GDv0LHuAZ"
      },
      "source": [
        "# Sanity Check for Dataset Size vs. Context Length\n",
        "\n",
        "Before training, we verify that both the training and validation datasets contain enough tokens to form at least one full sequence of the specified `context_length`.\n",
        "\n",
        "- We calculate the number of tokens allocated to each dataset by multiplying the total tokens by their respective ratios (`train_ratio` for training, and `1 - train_ratio` for validation).\n",
        "- If either dataset has fewer tokens than the model’s `context_length`, it means we cannot create even one complete input sequence for that set.\n",
        "- When this happens, a warning message is displayed advising to either:\n",
        "  - Decrease the `context_length`, or\n",
        "  - Adjust the `train_ratio` to allocate more tokens to the smaller dataset.\n",
        "\n",
        "This check helps ensure that our data loaders can properly generate batches for both training and validation without errors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWcz9_3RF_1E"
      },
      "outputs": [],
      "source": [
        "# Sanity check\n",
        "\n",
        "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the training loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"increase the `training_ratio`\")\n",
        "\n",
        "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the validation loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"decrease the `training_ratio`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB2vkRp9GAdz",
        "outputId": "7be57537-2f89-4200-adba-18db885d598d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([1, 256]) torch.Size([1, 256])\n",
            "53\n",
            "6\n"
          ]
        }
      ],
      "source": [
        "print(\"Train loader:\")\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(len(train_loader))\n",
        "print(len(val_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPQ5hLFYGDjS",
        "outputId": "e35770f0-7b49-44f1-a92c-c38ff0a6e69a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training tokens: 27136\n",
            "Validation tokens: 2816\n",
            "All tokens: 29952\n"
          ]
        }
      ],
      "source": [
        "train_tokens = 0\n",
        "for input_batch, target_batch in train_loader:\n",
        "    train_tokens += input_batch.numel()\n",
        "\n",
        "val_tokens = 0\n",
        "for input_batch, target_batch in val_loader:\n",
        "    val_tokens += input_batch.numel()\n",
        "\n",
        "print(\"Training tokens:\", train_tokens)\n",
        "print(\"Validation tokens:\", val_tokens)\n",
        "print(\"All tokens:\", train_tokens + val_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xLL7rQGIAXd"
      },
      "source": [
        "# Custom Components: LayerNorm, GELU Activation, and FeedForward Network\n",
        "\n",
        "This section defines key building blocks used in the GPT model architecture:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. `LayerNorm` (Layer Normalization)\n",
        "\n",
        "- **Purpose:**  \n",
        "  Normalizes the inputs across the embedding dimension to stabilize and speed up training.\n",
        "  \n",
        "- **Key Elements:**  \n",
        "  - `eps` (epsilon): A small value added for numerical stability during division.  \n",
        "  - `scale` (γ) and `shift` (β): Learnable parameters that allow the normalized output to be scaled and shifted, enabling the network to restore representational power if needed.\n",
        "  \n",
        "- **Forward Pass:**  \n",
        "  - Computes the mean and variance of the input `x` along the last dimension (embedding dimension).  \n",
        "  - Normalizes `x` by subtracting the mean and dividing by the standard deviation (square root of variance + epsilon).  \n",
        "  - Applies the learned scale and shift to the normalized output.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. `GELU` (Gaussian Error Linear Unit) Activation\n",
        "\n",
        "- **Purpose:**  \n",
        "  A smooth, non-linear activation function commonly used in transformers, which approximates the expected output of a stochastic regularization of inputs.\n",
        "  \n",
        "  \n",
        "- **Effect:**  \n",
        "  Allows small negative inputs to have a small positive output, enabling smoother gradients and better model performance compared to ReLU.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. `FeedForward` Neural Network Module\n",
        "\n",
        "- **Purpose:**  \n",
        "  Implements the position-wise feed-forward network inside each transformer block.\n",
        "  \n",
        "- **Architecture:**  \n",
        "  - **Expansion Layer:** Linear layer expanding the embedding dimension to four times larger (`4 * emb_dim`). This allows the network to learn more complex transformations.  \n",
        "  - **Activation:** Applies the `GELU` activation for non-linearity.  \n",
        "  - **Contraction Layer:** Projects the expanded features back to the original embedding dimension, preparing the output to be combined with residual connections.\n",
        "  \n",
        "- **Forward Pass:**  \n",
        "  - Passes the input tensor `x` through the sequential layers defined above, transforming the features accordingly.\n",
        "\n",
        "---\n",
        "\n",
        "These components together help the transformer learn rich representations while maintaining stable and efficient training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOW0qU6eGGFy"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
        "            GELU(), ## Activation\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbCd_0ONIa8h"
      },
      "source": [
        "# Multi-Head Attention Mechanism Explained\n",
        "\n",
        "Multi-Head Attention is a core component of the Transformer architecture that allows the model to focus on different parts of the input sequence simultaneously. This enhances the model’s ability to capture various relationships and dependencies in the data.\n",
        "\n",
        "---\n",
        "\n",
        "### What is Multi-Head Attention?\n",
        "\n",
        "- **Attention** computes a weighted average of values, where weights (attention scores) are determined by the compatibility between queries and keys.\n",
        "- Instead of a single attention mechanism, **multi-head attention** runs multiple attention operations (\"heads\") in parallel.  \n",
        "- Each head attends to different parts of the input, allowing the model to capture diverse contextual information.\n",
        "- The outputs of all heads are concatenated and linearly projected to form the final output.\n",
        "\n",
        "---\n",
        "\n",
        "### Breakdown of the Code Implementation\n",
        "\n",
        "- **Initialization:**\n",
        "  - `d_in`: Input embedding dimension.\n",
        "  - `d_out`: Output embedding dimension (must be divisible by the number of heads).\n",
        "  - `num_heads`: Number of parallel attention heads.\n",
        "  - `head_dim`: Dimension per attention head (`d_out // num_heads`).\n",
        "  - Three linear layers (`W_query`, `W_key`, `W_value`) transform inputs into queries, keys, and values.\n",
        "  - `out_proj` linearly projects concatenated head outputs back to `d_out`.\n",
        "  - A dropout layer is applied to attention weights for regularization.\n",
        "  - A **causal mask** is created to ensure each token only attends to previous tokens (important for autoregressive models like GPT).\n",
        "\n",
        "- **Forward Pass:**\n",
        "  1. Input `x` shape: `(batch_size, num_tokens, d_in)`.\n",
        "  2. Compute queries, keys, and values by applying the respective linear transformations.\n",
        "  3. Reshape each to `(batch_size, num_tokens, num_heads, head_dim)` to separate attention heads.\n",
        "  4. Transpose to `(batch_size, num_heads, num_tokens, head_dim)` for efficient batch matrix multiplication.\n",
        "  5. Calculate scaled dot-product attention scores\n",
        "  6. Apply the **causal mask** to prevent positions from attending to future tokens by masking out those attention scores (setting them to \\(-\\infty\\)).\n",
        "  7. Use `softmax` to convert masked scores into attention weights (probabilities).\n",
        "  8. Apply dropout on the attention weights for regularization.\n",
        "  9. Compute the weighted sum of values (`attn_weights @ values`), resulting in the context vector per head.\n",
        "  10. Transpose and reshape context vectors to concatenate heads back into shape `(batch_size, num_tokens, d_out)`.\n",
        "  11. Pass through the final linear projection (`out_proj`).\n",
        "\n",
        "---\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "- Multi-head attention enables the model to:\n",
        "  - Attend to multiple aspects of the sequence simultaneously.\n",
        "  - Capture different types of relationships (e.g., syntax, semantics).\n",
        "- The **causal mask** ensures the model maintains the autoregressive property needed for generative tasks, preventing \"cheating\" by looking ahead.\n",
        "\n",
        "---\n",
        "\n",
        "This implementation follows the original Transformer design, adapted for GPT-style autoregressive language modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfZ6p_S8GV1L"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed_Ek9M-JZLe"
      },
      "source": [
        "# Transformer Block Implementation\n",
        "\n",
        "This class implements a single Transformer block, which is a fundamental building unit of the GPT model architecture. Each block consists of two main sub-layers: Multi-Head Attention and Feed-Forward Neural Network, each followed by normalization and residual (shortcut) connections.\n",
        "\n",
        "---\n",
        "\n",
        "### Components:\n",
        "\n",
        "- **MultiHeadAttention (`self.att`):**  \n",
        "  Performs self-attention over the input tokens, allowing the model to weigh different positions in the sequence based on relevance.\n",
        "\n",
        "- **FeedForward (`self.ff`):**  \n",
        "  A position-wise fully connected network that applies non-linear transformations to each token independently, enabling complex feature extraction.\n",
        "\n",
        "- **LayerNorm (`self.norm1`, `self.norm2`):**  \n",
        "  Normalizes the input to each sub-layer to stabilize and accelerate training.\n",
        "\n",
        "- **Dropout (`self.drop_shortcut`):**  \n",
        "  Randomly zeros parts of the input during training to reduce overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### Forward Pass Flow:\n",
        "\n",
        "1. **Attention Block:**  \n",
        "   - Apply layer normalization to the input.  \n",
        "   - Pass normalized input through the multi-head attention mechanism.  \n",
        "   - Apply dropout for regularization.  \n",
        "   - Add the original input (shortcut) back to preserve information and improve gradient flow (residual connection).\n",
        "\n",
        "2. **Feed-Forward Block:**  \n",
        "   - Normalize the output of the attention block.  \n",
        "   - Pass through the feed-forward network.  \n",
        "   - Apply dropout again.  \n",
        "   - Add the input of this sub-layer back via another residual connection.\n",
        "\n",
        "---\n",
        "\n",
        "### Output Shape:\n",
        "\n",
        "- The input and output tensors maintain the shape `[batch_size, num_tokens, emb_dim]`.  \n",
        "- For example, with a batch size of 2, sequence length of 4, and embedding dimension of 768, the tensor shape is `(2, 4, 768)`.\n",
        "\n",
        "This design allows stacking multiple Transformer blocks to build deeper models capable of capturing complex language patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24CPTgQEGXhe"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        # 2*4*768\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "        # 2*4*768"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYx4bUjPJsgN"
      },
      "source": [
        "# GPT Model Architecture\n",
        "\n",
        "This class defines the full GPT model by stacking multiple Transformer blocks and adding token and positional embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "- **Token Embeddings (`self.tok_emb`):**  \n",
        "  Converts input token indices into dense vectors of size `emb_dim`. These embeddings represent the semantic meaning of tokens.\n",
        "\n",
        "- **Positional Embeddings (`self.pos_emb`):**  \n",
        "  Since Transformers process tokens in parallel, positional embeddings provide information about each token's position in the sequence, allowing the model to understand token order.\n",
        "\n",
        "- **Dropout on Embeddings (`self.drop_emb`):**  \n",
        "  Applies dropout regularization to the combined embeddings to prevent overfitting.\n",
        "\n",
        "- **Transformer Blocks (`self.trf_blocks`):**  \n",
        "  A sequence of `n_layers` Transformer blocks (defined previously), each containing multi-head attention and feed-forward sublayers with normalization and residual connections.\n",
        "\n",
        "- **Final LayerNorm (`self.final_norm`):**  \n",
        "  Normalizes the output of the last Transformer block before the final projection.\n",
        "\n",
        "- **Output Head (`self.out_head`):**  \n",
        "  A linear layer that projects the final hidden states to the vocabulary size, producing logits for each token prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### Forward Pass:\n",
        "\n",
        "1. Input token indices `in_idx` of shape `[batch_size, seq_len]` are converted to embeddings.\n",
        "2. Positional embeddings for each token position are generated and added to the token embeddings.\n",
        "3. The combined embeddings go through dropout.\n",
        "4. The sequence is passed through the stacked Transformer blocks.\n",
        "5. The output is normalized and then projected to logits representing predicted token probabilities.\n",
        "\n",
        "---\n",
        "\n",
        "### Additional Notes:\n",
        "\n",
        "- The model is initialized with a fixed random seed (`torch.manual_seed(123)`) for reproducibility.\n",
        "- The model is set to evaluation mode (`model.eval()`) to disable dropout during inference.\n",
        "\n",
        "This modular design enables the GPT model to learn contextual representations and generate coherent language sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "DYeimBZfGa_1",
        "outputId": "a67e65e1-80f5-4514-ef69-8f1580bb988d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1969103321.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGPTModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"emb_dim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"emb_dim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval();  # Disable dropout during inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNP29ms0KDut"
      },
      "source": [
        "# Loss Calculation Functions\n",
        "\n",
        "These functions compute the cross-entropy loss during training and evaluation for our GPT model.\n",
        "\n",
        "---\n",
        "\n",
        "### `calc_loss_batch`\n",
        "\n",
        "- **Purpose:** Computes the loss for a single batch of input and target sequences.\n",
        "- **Process:**\n",
        "  - Moves the input and target tensors to the specified device (CPU or GPU).\n",
        "  - Runs the model on the input batch to get the output logits.\n",
        "  - Calculates cross-entropy loss between the predicted logits and the target tokens.\n",
        "  - Flattens the logits and target tensors to align dimensions for loss computation.\n",
        "- **Returns:** The loss value for that batch.\n",
        "\n",
        "---\n",
        "\n",
        "### `calc_loss_loader`\n",
        "\n",
        "- **Purpose:** Computes the average loss over multiple batches from a data loader.\n",
        "- **Process:**\n",
        "  - Iterates over the data loader up to a specified number of batches (`num_batches`).\n",
        "  - Calls `calc_loss_batch` for each batch and accumulates the loss.\n",
        "  - Handles cases where the data loader might be empty or the requested number of batches exceeds available batches.\n",
        "- **Returns:** The average loss across the evaluated batches.\n",
        "\n",
        "---\n",
        "\n",
        "These functions help monitor model performance during training and validation by providing batch-level and epoch-level loss metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XluLoWDoGgEY"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EccsEr-gGkeV",
        "outputId": "e5f302d0-282b-416b-8d39-5979bc56082a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 10.961539862290868\n",
            "Validation loss: 10.966237386067709\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Note:\n",
        "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
        "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
        "# However, the resulting loss values may be slightly different.\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    device = torch.device(\"cuda\")\n",
        "#elif torch.backends.mps.is_available():\n",
        "#    device = torch.device(\"mps\")\n",
        "#else:\n",
        "#    device = torch.device(\"cpu\")\n",
        "#\n",
        "# print(f\"Using {device} device.\")\n",
        "\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_THVvWGG8NG"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh-BXufiHUSV"
      },
      "outputs": [],
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlbQi9YpHgh9"
      },
      "outputs": [],
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "\n",
        "    ###Input batch:\n",
        " ###tensor([[6109, 3626, 6100,  345],\n",
        "        ##[6109, 1110, 6622,  257]])\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNZ0b2JVG_h9"
      },
      "outputs": [],
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kkUs3CtKQq7"
      },
      "source": [
        "# Simple Training Loop for GPT Model\n",
        "\n",
        "This function manages the training process of the GPT model over multiple epochs, including periodic evaluation and sample text generation.\n",
        "\n",
        "---\n",
        "\n",
        "### Function Inputs:\n",
        "\n",
        "- `model`: The GPT model to be trained.\n",
        "- `train_loader`: DataLoader for the training dataset.\n",
        "- `val_loader`: DataLoader for the validation dataset.\n",
        "- `optimizer`: Optimization algorithm used to update model weights.\n",
        "- `device`: Computing device (CPU or GPU).\n",
        "- `num_epochs`: Number of full passes through the training dataset.\n",
        "- `eval_freq`: Frequency (in training steps) to run evaluation.\n",
        "- `eval_iter`: Number of batches used during evaluation.\n",
        "- `start_context`: Initial text context for sample generation.\n",
        "- `tokenizer`: Tokenizer to encode/decode text for generation.\n",
        "\n",
        "---\n",
        "\n",
        "### Training Workflow:\n",
        "\n",
        "1. **Initialize Tracking Variables:**  \n",
        "   Lists to store training/validation losses and the number of tokens processed.\n",
        "\n",
        "2. **Epoch Loop:**  \n",
        "   Runs for the specified number of epochs.\n",
        "   - Sets the model to training mode (`model.train()`).\n",
        "\n",
        "3. **Batch Loop:**  \n",
        "   For each batch in the training data:\n",
        "   - Reset gradients (`optimizer.zero_grad()`).\n",
        "   - Compute loss on the batch (`calc_loss_batch`).\n",
        "   - Perform backpropagation (`loss.backward()`).\n",
        "   - Update model weights (`optimizer.step()`).\n",
        "   - Track the total number of tokens processed and the global training step count.\n",
        "\n",
        "4. **Evaluation:**  \n",
        "   Every `eval_freq` steps:\n",
        "   - Evaluate the model on training and validation data (`evaluate_model`).\n",
        "   - Record losses and tokens seen.\n",
        "   - Print current training progress.\n",
        "\n",
        "5. **Sample Generation:**  \n",
        "   After each epoch, generate and print a sample text to qualitatively assess the model’s learning progress.\n",
        "\n",
        "---\n",
        "\n",
        "### Returns:\n",
        "\n",
        "- `train_losses`: List of training losses recorded during evaluation.\n",
        "- `val_losses`: List of validation losses recorded during evaluation.\n",
        "- `track_tokens_seen`: List tracking the number of tokens processed at each evaluation point.\n",
        "\n",
        "---\n",
        "\n",
        "This function provides a straightforward yet effective training loop with monitoring and sample generation to observe the model's behavior over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW_DNJNUGmzF"
      },
      "outputs": [],
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX8I4mI5KW0t"
      },
      "source": [
        "# Model Training Setup and Execution Time Tracking\n",
        "\n",
        "---\n",
        "\n",
        "### Overview:\n",
        "\n",
        "- **Seed Initialization:**  \n",
        "  Sets a fixed random seed (`torch.manual_seed(123)`) to ensure reproducibility of the training results.\n",
        "\n",
        "- **Model and Optimizer:**  \n",
        "  - Instantiates the GPT model with the defined configuration (`GPT_CONFIG_124M`).  \n",
        "  - Moves the model to the specified device (CPU or GPU).  \n",
        "  - Initializes the AdamW optimizer with a learning rate of 0.0004 and weight decay of 0.1 for regularization.\n",
        "\n",
        "- **Training Parameters:**  \n",
        "  - Runs training for 20 epochs.  \n",
        "  - Evaluates training and validation loss every 5 steps, using 5 batches per evaluation.  \n",
        "  - Starts sample text generation with the prompt `\"Every effort moves you\"`.  \n",
        "  - Uses the specified tokenizer for encoding/decoding text.\n",
        "\n",
        "- **Execution Time Tracking (Optional):**  \n",
        "  - Records the start time before training begins.  \n",
        "  - Records the end time after training completes.  \n",
        "  - Calculates and prints the total training time in minutes.\n",
        "\n",
        "---\n",
        "\n",
        "This setup allows for controlled training with periodic evaluation and performance monitoring, while optionally measuring the total runtime for convenience.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACGiPM3THB5e",
        "outputId": "99e6d8ab-baf0-435f-c174-b90cd0d1d1ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.935, Val loss 9.897\n",
            "Ep 1 (Step 000005): Train loss 8.416, Val loss 8.505\n",
            "Ep 1 (Step 000010): Train loss 7.513, Val loss 7.624\n",
            "Ep 1 (Step 000015): Train loss 6.943, Val loss 7.309\n",
            "Ep 1 (Step 000020): Train loss 6.761, Val loss 7.207\n",
            "Ep 1 (Step 000025): Train loss 6.812, Val loss 7.089\n",
            "Ep 1 (Step 000030): Train loss 6.610, Val loss 7.007\n",
            "Ep 1 (Step 000035): Train loss 6.177, Val loss 6.928\n",
            "Ep 1 (Step 000040): Train loss 6.211, Val loss 6.886\n",
            "Ep 1 (Step 000045): Train loss 6.273, Val loss 6.788\n",
            "Ep 1 (Step 000050): Train loss 6.011, Val loss 6.746\n",
            "Every effort moves you, and a little, and a little, and a little, and a little, and a little, and a little, and a little, and the moon, and a little, and a little, a little, and the moon, and a\n",
            "Ep 2 (Step 000055): Train loss 5.803, Val loss 6.718\n",
            "Ep 2 (Step 000060): Train loss 5.815, Val loss 6.664\n",
            "Ep 2 (Step 000065): Train loss 5.709, Val loss 6.708\n",
            "Ep 2 (Step 000070): Train loss 5.502, Val loss 6.533\n",
            "Ep 2 (Step 000075): Train loss 5.391, Val loss 6.545\n",
            "Ep 2 (Step 000080): Train loss 5.243, Val loss 6.491\n",
            "Ep 2 (Step 000085): Train loss 5.093, Val loss 6.452\n",
            "Ep 2 (Step 000090): Train loss 4.932, Val loss 6.431\n",
            "Ep 2 (Step 000095): Train loss 5.126, Val loss 6.360\n",
            "Ep 2 (Step 000100): Train loss 4.977, Val loss 6.323\n",
            "Ep 2 (Step 000105): Train loss 4.798, Val loss 6.338\n",
            "Every effort moves you, and the tree.      The the tree was a little. The mist. The air. The air was a little the tree. The mist, and the tree was a little the tree. The air. They had been\n",
            "Ep 3 (Step 000110): Train loss 4.687, Val loss 6.309\n",
            "Ep 3 (Step 000115): Train loss 4.338, Val loss 6.348\n",
            "Ep 3 (Step 000120): Train loss 4.210, Val loss 6.308\n",
            "Ep 3 (Step 000125): Train loss 4.366, Val loss 6.285\n",
            "Ep 3 (Step 000130): Train loss 4.235, Val loss 6.264\n",
            "Ep 3 (Step 000135): Train loss 4.225, Val loss 6.233\n",
            "Ep 3 (Step 000140): Train loss 4.038, Val loss 6.197\n",
            "Ep 3 (Step 000145): Train loss 3.827, Val loss 6.185\n",
            "Ep 3 (Step 000150): Train loss 4.052, Val loss 6.159\n",
            "Ep 3 (Step 000155): Train loss 3.998, Val loss 6.147\n",
            "Every effort moves you, but a little, but it made of the moonbeam”                                   The\n",
            "Ep 4 (Step 000160): Train loss 3.566, Val loss 6.113\n",
            "Ep 4 (Step 000165): Train loss 3.451, Val loss 6.108\n",
            "Ep 4 (Step 000170): Train loss 3.576, Val loss 6.164\n",
            "Ep 4 (Step 000175): Train loss 3.562, Val loss 6.170\n",
            "Ep 4 (Step 000180): Train loss 3.127, Val loss 6.131\n",
            "Ep 4 (Step 000185): Train loss 3.201, Val loss 6.126\n",
            "Ep 4 (Step 000190): Train loss 3.166, Val loss 6.138\n",
            "Ep 4 (Step 000195): Train loss 2.659, Val loss 6.164\n",
            "Ep 4 (Step 000200): Train loss 2.685, Val loss 6.165\n",
            "Ep 4 (Step 000205): Train loss 2.853, Val loss 6.094\n",
            "Ep 4 (Step 000210): Train loss 3.050, Val loss 6.077\n",
            "Every effort moves you, and the tree.   The moonbeam’s whispers were like a tiny, and the tree. The tree. The night. The air was a little while, and the tree’s a very small, and the air\n",
            "Ep 5 (Step 000215): Train loss 2.747, Val loss 6.078\n",
            "Ep 5 (Step 000220): Train loss 2.574, Val loss 6.108\n",
            "Ep 5 (Step 000225): Train loss 2.247, Val loss 6.150\n",
            "Ep 5 (Step 000230): Train loss 2.463, Val loss 6.166\n",
            "Ep 5 (Step 000235): Train loss 2.422, Val loss 6.222\n",
            "Ep 5 (Step 000240): Train loss 2.027, Val loss 6.193\n",
            "Ep 5 (Step 000245): Train loss 2.394, Val loss 6.190\n",
            "Ep 5 (Step 000250): Train loss 2.018, Val loss 6.217\n",
            "Ep 5 (Step 000255): Train loss 2.070, Val loss 6.239\n",
            "Ep 5 (Step 000260): Train loss 2.129, Val loss 6.232\n",
            "Every effort moves you see.\"  He found of the moonbeam turned, he had a goodnight, it was time for a moment, a magical moonbeam too, but he knew it was just the moonbeam seemed to the moonbeam. The moonbeam.\n",
            "Ep 6 (Step 000265): Train loss 1.807, Val loss 6.217\n",
            "Ep 6 (Step 000270): Train loss 1.756, Val loss 6.255\n",
            "Ep 6 (Step 000275): Train loss 1.665, Val loss 6.337\n",
            "Ep 6 (Step 000280): Train loss 1.448, Val loss 6.316\n",
            "Ep 6 (Step 000285): Train loss 1.432, Val loss 6.351\n",
            "Ep 6 (Step 000290): Train loss 1.126, Val loss 6.344\n",
            "Ep 6 (Step 000295): Train loss 1.468, Val loss 6.372\n",
            "Ep 6 (Step 000300): Train loss 1.247, Val loss 6.395\n",
            "Ep 6 (Step 000305): Train loss 1.206, Val loss 6.401\n",
            "Ep 6 (Step 000310): Train loss 1.507, Val loss 6.401\n",
            "Ep 6 (Step 000315): Train loss 1.284, Val loss 6.442\n",
            "Every effort moves you thank a little bolder.  The moonbeam itself seemed to delight in the pond. He felt like a tiny, a magical moonbeam too, as the night, his paws making them with the stream’s heartbeat? He felt a\n",
            "Ep 7 (Step 000320): Train loss 1.067, Val loss 6.488\n",
            "Ep 7 (Step 000325): Train loss 0.929, Val loss 6.545\n",
            "Ep 7 (Step 000330): Train loss 0.835, Val loss 6.536\n",
            "Ep 7 (Step 000335): Train loss 0.791, Val loss 6.539\n",
            "Ep 7 (Step 000340): Train loss 0.880, Val loss 6.609\n",
            "Ep 7 (Step 000345): Train loss 0.649, Val loss 6.660\n",
            "Ep 7 (Step 000350): Train loss 0.697, Val loss 6.687\n",
            "Ep 7 (Step 000355): Train loss 0.732, Val loss 6.658\n",
            "Ep 7 (Step 000360): Train loss 0.559, Val loss 6.669\n",
            "Ep 7 (Step 000365): Train loss 0.578, Val loss 6.676\n",
            "Ep 7 (Step 000370): Train loss 0.592, Val loss 6.688\n",
            "Every effort moves you thank a little bolder.  The moonbeam itself seemed to delight in the pond. It skimmed across the moonbeam pulsed once, its light pulsed. It was still, his tiny feet barely seeming to drift away like leaves on\n",
            "Ep 8 (Step 000375): Train loss 0.477, Val loss 6.753\n",
            "Ep 8 (Step 000380): Train loss 0.554, Val loss 6.774\n",
            "Ep 8 (Step 000385): Train loss 0.522, Val loss 6.844\n",
            "Ep 8 (Step 000390): Train loss 0.512, Val loss 6.807\n",
            "Ep 8 (Step 000395): Train loss 0.465, Val loss 6.900\n",
            "Ep 8 (Step 000400): Train loss 0.585, Val loss 6.863\n",
            "Ep 8 (Step 000405): Train loss 0.407, Val loss 6.923\n",
            "Ep 8 (Step 000410): Train loss 0.375, Val loss 6.852\n",
            "Ep 8 (Step 000415): Train loss 0.354, Val loss 6.935\n",
            "Ep 8 (Step 000420): Train loss 0.376, Val loss 6.955\n",
            "Every effort moves you thank a beam of light? He simply chittered softly, hoping it understood.  As if in response, to make special \"fish-cicles\" – tiny, he noticed several enormous lily pads floating near the air. \"I hope\n",
            "Ep 9 (Step 000425): Train loss 0.348, Val loss 6.952\n",
            "Ep 9 (Step 000430): Train loss 0.426, Val loss 6.995\n",
            "Ep 9 (Step 000435): Train loss 0.335, Val loss 7.098\n",
            "Ep 9 (Step 000440): Train loss 0.280, Val loss 6.955\n",
            "Ep 9 (Step 000445): Train loss 0.335, Val loss 6.944\n",
            "Ep 9 (Step 000450): Train loss 0.261, Val loss 7.062\n",
            "Ep 9 (Step 000455): Train loss 0.223, Val loss 7.048\n",
            "Ep 9 (Step 000460): Train loss 0.163, Val loss 7.054\n",
            "Ep 9 (Step 000465): Train loss 0.246, Val loss 7.108\n",
            "Ep 9 (Step 000470): Train loss 0.151, Val loss 7.045\n",
            "Ep 9 (Step 000475): Train loss 0.200, Val loss 7.079\n",
            "Every effort moves you thank a beam of light? He simply chittered softly, hoping it understood.  As if in response, the moonbeam pulsed once, a gentle farewell, it was over him kindly, as they slept soundly, their own outstretched\n",
            "Ep 10 (Step 000480): Train loss 0.206, Val loss 7.135\n",
            "Ep 10 (Step 000485): Train loss 0.203, Val loss 7.253\n",
            "Ep 10 (Step 000490): Train loss 0.152, Val loss 7.221\n",
            "Ep 10 (Step 000495): Train loss 0.181, Val loss 7.153\n",
            "Ep 10 (Step 000500): Train loss 0.182, Val loss 7.192\n",
            "Ep 10 (Step 000505): Train loss 0.173, Val loss 7.192\n",
            "Ep 10 (Step 000510): Train loss 0.149, Val loss 7.217\n",
            "Ep 10 (Step 000515): Train loss 0.115, Val loss 7.201\n",
            "Ep 10 (Step 000520): Train loss 0.128, Val loss 7.174\n",
            "Ep 10 (Step 000525): Train loss 0.117, Val loss 7.191\n",
            "Every effort moves you thank a beam of light? He simply chittered softly, hoping it understood.  As if in response, the moonbeam pulsed once, a gentle farewell, and then, as the first, faintest blush of dawn touched the eastern sky\n",
            "Ep 11 (Step 000530): Train loss 0.133, Val loss 7.205\n",
            "Ep 11 (Step 000535): Train loss 0.158, Val loss 7.283\n",
            "Ep 11 (Step 000540): Train loss 0.091, Val loss 7.345\n",
            "Ep 11 (Step 000545): Train loss 0.143, Val loss 7.276\n",
            "Ep 11 (Step 000550): Train loss 0.116, Val loss 7.306\n",
            "Ep 11 (Step 000555): Train loss 0.114, Val loss 7.304\n",
            "Ep 11 (Step 000560): Train loss 0.116, Val loss 7.317\n",
            "Ep 11 (Step 000565): Train loss 0.087, Val loss 7.343\n",
            "Ep 11 (Step 000570): Train loss 0.097, Val loss 7.354\n",
            "Ep 11 (Step 000575): Train loss 0.113, Val loss 7.333\n",
            "Ep 11 (Step 000580): Train loss 0.101, Val loss 7.325\n",
            "Every effort moves you thank a beam of light? He simply chittered softly, hoping it understood.  As if in response, the moonbeam pulsed once, a gentle farewell, and then, as the first, faintest blush of dawn touched the eastern sky\n",
            "Ep 12 (Step 000585): Train loss 0.075, Val loss 7.361\n",
            "Ep 12 (Step 000590): Train loss 0.082, Val loss 7.434\n",
            "Ep 12 (Step 000595): Train loss 0.086, Val loss 7.493\n",
            "Ep 12 (Step 000600): Train loss 0.066, Val loss 7.451\n",
            "Ep 12 (Step 000605): Train loss 0.058, Val loss 7.422\n",
            "Ep 12 (Step 000610): Train loss 0.103, Val loss 7.474\n",
            "Ep 12 (Step 000615): Train loss 0.057, Val loss 7.478\n",
            "Ep 12 (Step 000620): Train loss 0.054, Val loss 7.514\n",
            "Ep 12 (Step 000625): Train loss 0.060, Val loss 7.465\n",
            "Ep 12 (Step 000630): Train loss 0.041, Val loss 7.404\n",
            "Ep 12 (Step 000635): Train loss 0.056, Val loss 7.468\n",
            "Every effort moves you thank a beam of light? He simply chittered softly, hoping it understood.  As if in response, the moonbeam pulsed once, a gentle farewell, and then, as the first, faintest blush of dawn touched the eastern sky\n",
            "Ep 13 (Step 000640): Train loss 0.050, Val loss 7.587\n",
            "Ep 13 (Step 000645): Train loss 0.052, Val loss 7.535\n",
            "Ep 13 (Step 000650): Train loss 0.059, Val loss 7.512\n",
            "Ep 13 (Step 000655): Train loss 0.043, Val loss 7.566\n",
            "Ep 13 (Step 000660): Train loss 0.050, Val loss 7.581\n",
            "Ep 13 (Step 000665): Train loss 0.044, Val loss 7.537\n",
            "Ep 13 (Step 000670): Train loss 0.033, Val loss 7.527\n",
            "Ep 13 (Step 000675): Train loss 0.037, Val loss 7.585\n",
            "Ep 13 (Step 000680): Train loss 0.035, Val loss 7.545\n",
            "Ep 13 (Step 000685): Train loss 0.037, Val loss 7.571\n",
            "Every effort moves you thank a beam of light? He simply chittered softly, hoping it understood.  As if in response, the moonbeam pulsed once, a gentle farewell, and then, as the first, faintest blush of dawn touched the eastern sky\n",
            "Ep 14 (Step 000690): Train loss 0.022, Val loss 7.678\n",
            "Ep 14 (Step 000695): Train loss 0.029, Val loss 7.653\n",
            "Ep 14 (Step 000700): Train loss 0.024, Val loss 7.606\n",
            "Ep 14 (Step 000705): Train loss 0.032, Val loss 7.602\n",
            "Ep 14 (Step 000710): Train loss 0.036, Val loss 7.612\n",
            "Ep 14 (Step 000715): Train loss 0.028, Val loss 7.610\n",
            "Ep 14 (Step 000720): Train loss 0.026, Val loss 7.628\n",
            "Ep 14 (Step 000725): Train loss 0.031, Val loss 7.668\n",
            "Ep 14 (Step 000730): Train loss 0.020, Val loss 7.622\n",
            "Ep 14 (Step 000735): Train loss 0.027, Val loss 7.628\n",
            "Ep 14 (Step 000740): Train loss 0.022, Val loss 7.670\n",
            "Every effort moves you thank a beam of light? He simply chittered softly, hoping it understood. It was like learning a new, the moonbeam pulsed once, a gentle farewell, and then, as the first, faintest blush of dawn touched the eastern sky\n",
            "Ep 15 (Step 000745): Train loss 0.018, Val loss 7.656\n",
            "Ep 15 (Step 000750): Train loss 0.031, Val loss 7.628\n",
            "Ep 15 (Step 000755): Train loss 0.018, Val loss 7.678\n",
            "Ep 15 (Step 000760): Train loss 0.017, Val loss 7.727\n",
            "Ep 15 (Step 000765): Train loss 0.021, Val loss 7.713\n",
            "Ep 15 (Step 000770): Train loss 0.022, Val loss 7.706\n",
            "Ep 15 (Step 000775): Train loss 0.020, Val loss 7.705\n",
            "Ep 15 (Step 000780): Train loss 0.016, Val loss 7.695\n",
            "Ep 15 (Step 000785): Train loss 0.019, Val loss 7.689\n",
            "Ep 15 (Step 000790): Train loss 0.016, Val loss 7.673\n",
            "Every effort moves you thank a beam of light? He simply chittered softly, hoping it understood.  As if in response, the moonbeam pulsed once, a gentle farewell, and then, as the first, faintest blush of dawn touched the eastern sky\n",
            "Ep 16 (Step 000795): Train loss 0.018, Val loss 7.677\n",
            "Ep 16 (Step 000800): Train loss 0.017, Val loss 7.675\n",
            "Ep 16 (Step 000805): Train loss 0.021, Val loss 7.704\n",
            "Ep 16 (Step 000810): Train loss 0.020, Val loss 7.694\n",
            "Ep 16 (Step 000815): Train loss 0.016, Val loss 7.648\n",
            "Ep 16 (Step 000820): Train loss 0.021, Val loss 7.660\n",
            "Ep 16 (Step 000825): Train loss 0.009, Val loss 7.708\n",
            "Ep 16 (Step 000830): Train loss 0.012, Val loss 7.726\n",
            "Ep 16 (Step 000835): Train loss 0.019, Val loss 7.705\n",
            "Ep 16 (Step 000840): Train loss 0.013, Val loss 7.697\n",
            "Ep 16 (Step 000845): Train loss 0.015, Val loss 7.653\n",
            "Every effort moves you thank a beam of light? He simply chittered softly, hoping it understood.  As if in response, the moonbeam pulsed once, a gentle farewell, and then, as the first, faintest blush of dawn touched the eastern sky\n",
            "Ep 17 (Step 000850): Train loss 0.013, Val loss 7.649\n",
            "Ep 17 (Step 000855): Train loss 0.015, Val loss 7.722\n",
            "Ep 17 (Step 000860): Train loss 0.011, Val loss 7.753\n",
            "Ep 17 (Step 000865): Train loss 0.017, Val loss 7.710\n",
            "Ep 17 (Step 000870): Train loss 0.020, Val loss 7.677\n",
            "Ep 17 (Step 000875): Train loss 0.022, Val loss 7.664\n",
            "Ep 17 (Step 000880): Train loss 0.012, Val loss 7.679\n",
            "Ep 17 (Step 000885): Train loss 0.015, Val loss 7.741\n",
            "Ep 17 (Step 000890): Train loss 0.012, Val loss 7.758\n",
            "Ep 17 (Step 000895): Train loss 0.011, Val loss 7.764\n",
            "Ep 17 (Step 000900): Train loss 0.015, Val loss 7.761\n",
            "Every effort moves you thank a beam of light? He simply chittered softly, hoping it understood.  As if in response, the moonbeam pulsed once, a gentle farewell, and then, as the first, faintest blush of dawn touched the eastern sky\n",
            "Ep 18 (Step 000905): Train loss 0.014, Val loss 7.788\n",
            "Ep 18 (Step 000910): Train loss 0.013, Val loss 7.800\n",
            "Ep 18 (Step 000915): Train loss 0.014, Val loss 7.776\n",
            "Ep 18 (Step 000920): Train loss 0.018, Val loss 7.786\n",
            "Ep 18 (Step 000925): Train loss 0.015, Val loss 7.773\n",
            "Ep 18 (Step 000930): Train loss 0.014, Val loss 7.766\n",
            "Ep 18 (Step 000935): Train loss 0.010, Val loss 7.798\n",
            "Ep 18 (Step 000940): Train loss 0.012, Val loss 7.805\n",
            "Ep 18 (Step 000945): Train loss 0.012, Val loss 7.772\n",
            "Ep 18 (Step 000950): Train loss 0.015, Val loss 7.766\n",
            "Every effort moves you thank a beam of light? He simply chittered softly, hoping it understood.  As if in response, the moonbeam pulsed once, a gentle farewell, and then, as the first, faintest blush of dawn touched the eastern sky\n",
            "Ep 19 (Step 000955): Train loss 0.008, Val loss 7.802\n",
            "Ep 19 (Step 000960): Train loss 0.014, Val loss 7.791\n",
            "Ep 19 (Step 000965): Train loss 0.010, Val loss 7.814\n",
            "Ep 19 (Step 000970): Train loss 0.013, Val loss 7.775\n",
            "Ep 19 (Step 000975): Train loss 0.009, Val loss 7.756\n",
            "Ep 19 (Step 000980): Train loss 0.008, Val loss 7.742\n",
            "Ep 19 (Step 000985): Train loss 0.018, Val loss 7.794\n",
            "Ep 19 (Step 000990): Train loss 0.005, Val loss 7.797\n",
            "Ep 19 (Step 000995): Train loss 0.012, Val loss 7.802\n",
            "Ep 19 (Step 001000): Train loss 0.008, Val loss 7.802\n",
            "Ep 19 (Step 001005): Train loss 0.019, Val loss 7.742\n",
            "Every effort moves you thank a beam of light? He simply chittered softly, hoping it understood.  As if in response, the moonbeam pulsed once, a gentle farewell, and then, as the first, faintest blush of dawn touched the eastern sky\n",
            "Ep 20 (Step 001010): Train loss 0.014, Val loss 7.736\n",
            "Ep 20 (Step 001015): Train loss 0.010, Val loss 7.809\n",
            "Ep 20 (Step 001020): Train loss 0.013, Val loss 7.814\n",
            "Ep 20 (Step 001025): Train loss 0.025, Val loss 7.827\n",
            "Ep 20 (Step 001030): Train loss 0.019, Val loss 7.831\n",
            "Ep 20 (Step 001035): Train loss 0.014, Val loss 7.872\n",
            "Ep 20 (Step 001040): Train loss 0.013, Val loss 7.722\n",
            "Ep 20 (Step 001045): Train loss 0.018, Val loss 7.727\n",
            "Ep 20 (Step 001050): Train loss 0.012, Val loss 7.726\n",
            "Ep 20 (Step 001055): Train loss 0.018, Val loss 7.714\n",
            "Every effort moves you thank a beam of light? He simply chittered softly, hoping it understood.  As if in response, the moonbeam pulsed once, a gentle farewell, and then, as the first, faintest blush of dawn touched the eastern sky\n",
            "Training completed in 5.38 minutes.\n"
          ]
        }
      ],
      "source": [
        "# Note:\n",
        "# Uncomment the following code to calculate the execution time\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 20\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Note:\n",
        "# Uncomment the following code to show the execution time\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "zrpp5rxJHrGh",
        "outputId": "0b87cdf5-fcea-4a8a-805e-fa248f766f11"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mkuXv6pNas_"
      },
      "source": [
        "The validation loss showed some initial improvement but then started to increase. This behavior can be addressed using techniques like temperature scaling or controlling randomness during training. However, these approaches are beyond the scope of the current experiment and will be explored in future work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U9a6KUYNogs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
